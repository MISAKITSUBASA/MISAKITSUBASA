# ðŸ‘‹ Hi, I'm Qian Tang (å”éªž)

ðŸ§  CS Master's @ NYU Courant  
ðŸŽ“ Previously @ University of Toronto â€” ECE (MEng) & CS/Math/Stats (BSc)  
ðŸ§© LLM Infrastructure Â· ML Systems Â· Applied Model Architecture Â· Language x Reasoning

---

## ðŸŒ Who I Am

I'm an engineerâ€“scientist hybrid working at the intersection of **foundation model research and real-world deployment**.  
I care about building ML systems that arenâ€™t just accurate â€” but **aware, adaptive, and architecturally honest**.

In short:  
> I build intelligent systems that help large models think clearly, serve responsibly, and evolve safely â€” under real-world constraints.

---

## ðŸ§  What I Work On

My focus spans three deeply interconnected tracks:

### ðŸ”§ ML Infrastructure  
How do we serve, route, and observe LLMs at production scale?  
I design low-latency, fault-tolerant pipelines for semantic retrieval, prompt orchestration, fallback reasoning, and caching.  
My systems aim to make large models **feel instant, grounded, and controllable** â€” without compromising flexibility.

### ðŸ§¬ Model Architecture  
How do we make LLMs smarter, leaner, and more human-aligned?  
I explore hybrid architectures:  
- **Soft contrastive learning** to improve alignment under ambiguity  
- **ViT and cross-modal models** for structure-aware learning  
- **Multi-exit transformers** and **token-pruned inference paths** to reduce latency while preserving reasoning depth  
I care about **making models more efficient not just by compression, but by structure**.

### ðŸ” Feedback-Driven ML  
How do we help models learn post-deployment â€” from users, agents, and failure cases?  
I design schema and pipelines for **real-time feedback**, **confidence-based escalation**, and **continuous evaluation** tied to real outcomes.  
Think: model predictions as first drafts, and the world as its annotator.

---

## ðŸš§ What I'm Working Toward

I donâ€™t just ship models â€” Iâ€™m building toward:

- ðŸ”Ž **Interpretable, multi-stage language systems** that can reflect, reroute, or defer
- âš™ï¸ **Robust AI infrastructure** where caching, degradation, and feedback are first-class citizens
- ðŸ§  **Efficient model architectures** that balance latency, alignment, and reasoning
- ðŸ”„ **Closed-loop AI workflows** where human edits and business signals improve the next prediction

I want to engineer systems where models donâ€™t just â€œperformâ€, but **adapt and justify**.

---

## ðŸ“« Get in Touch

- âœ‰ï¸ [qian.tang.1999@outlook.com](mailto:qian.tang.1999@outlook.com) (Work)  
- ðŸŽ“ [qt2118@nyu.edu](mailto:qt2118@nyu.edu) (Academic)

---

> *I donâ€™t just run models. I build the systems around them that make them useful â€” and the feedback loops that make them better.*
